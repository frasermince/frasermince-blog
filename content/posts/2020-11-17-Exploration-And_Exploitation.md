---
title: "Exploration and Exploitation"
date: "2020-11-11T10:00:00.0000"
template: "post"
draft: false
slug: "reinforcement-learning-2"
category: "Machine Learning"
tags:
  - "Machine Learning"
  - "Reinforcement Learning"
description: ""
socialImage: "/media/image-2.jpg"
---

Moving past the basics I now want to explore a tradeoff in Reinforcement
Learning. This is the tradeoff between exploration and exploitation. Whenever we
are dealing with a reinforcement learning there is a choice we have to make. Do
we take advantage of the most valuable choices we know about, or do we instead
search for new, potentially more valuable choices. Through exploitation we can
maximize our performance of the problem but through exploration we can increase
our knowledge.

We can depict how we make this tradeoff by simplifying our environment. We do
this by taking away the sequential structure of our environment and assume that
past actions do not effect our current environment state.

For example we have a rat that can pull one of two levers. When he pulls a lever
he either gets an electric shock or some cheese. Each lever has a certain
probability of giving a shock and a certain probability of giving cheese. Let's
say the rat pulls the first lever on the first day and gets a shock. He pulls
the second layer on the second day and gets some cheese. On the third day he
again pulls the second layer and gets some shock. What should he do next? While
you may have a theory for what the rat should do next the answer is non trivial.
Should the rat keep going to the lever that led to a reward once? Or further
explore the one that hasn't given a reward yet?

As we consider this tradeoff it becomes clear to maximize the long term strategy
we may have to make short term sacrifices. For example we may at times not
choose the decision we think is optimal in order to gather new information. This
is also a byproduct of online decision making. Where we are collecting data and
making decisions at the same time.

We call the setting in which we discuss this tradeoff the Multi-Armed Bandit.
While this may sound like some boss fight in a Final Fantasy game the word
bandit here comes from one armed bandit which is another name for a slot
machine. The basic idea of the multi-armed bandit is a row of slot machines each
having it's own fixed distribution that is unknown. Based on these distributions
the slot machines will payoff. We view which slot machine to play at a given
timestep as the action choice. Once we choose a slot machine we get some amount
of reward this reward is assumed to be random, hence our distribution. One nice
thing about this problem is it simplifies the problem of RL by not worrying
about sequentially or what actions were taken at previous timesteps. This is due
to the fixed nature of the distributions. This information ceases to matter in
this problem. Our goal is to maximize not rewards at any given timestep but
instead cumulative rewards. What this means in practice is we don't want to lose
too much value in the learning process. We want to be able to learn in the
optimal way possible.

Due to not having any environmental state to inform our action we can simplify
the q_a value to just being condition on action and not on state. This could
also be expressed as finding values only where an action is chosen.

$$
q(a) = E[R_t | A_t = a]
$$

$$
Q_t(a) = \dfrac{\sum_{n=1}^{t}R_n\mathcal{I}(A_n = a)}{\sum_{n=1}^{t}\mathcal{I}(A_n = a)}
$$

This $\mathcal{I}$ refers to the indicator function. This function is 1 when the action
occurs and 0 otherwise. This means that we only look at values where the action
occurs.

This can also be treated incrementally.
$$
Q_t(A_t) = Q_{t-l}(A_t) + α_t(R_t - Q_{t-1}(A_t))
$$
This $Q_t$ no longer refers to the true value but rather an estimate.

The α is the step size. Often we set the step size to 1/number of steps. This
gives us an average. We could also use a constant step size meaning we converge
towards a value instead of averaging. This is useful for function
approximations like neural nets.


In this case at every step we are finding the difference between our reward
currently and our past value. This acts as an error function when multiplied by
the step size alpha and added to the current previous value. 
This is derived using simple algebra starting with:
$$
Q_{n+1} = \dfrac{1}{n}\sum_{i=1}^{n}(R_i)
$$
And solving to define $Q_{n+1}$ in terms of $Q_n$. I won't repeat this process
here but if you are interested in how the above incremental formula is derived I
strongly suggest you check out page 24 of [Sutton And Barto](http://incompleteideas.net/book/bookdraft2017nov5.pdf).

The next dilemma we have to face is over when do we stop greedily getting what
seems like the best value?

Cheese = 1
Shock = -1

1. Pull black lever: shock
2. Pull white lever: cheese
3. Pull white lever: shock

$$
Q_3(white) = 0
$$
$$
Q_3(black) = -1
$$

Now let's assume the following:
4. Pull white lever: shock
5. Pull white lever: shock
6. Pull white lever: shock

We now have:

$$
Q_3(white) = 0
$$
$$
Q_3(black) = -1
$$

You will notice as we are shocked more the $Q_3(white)$ decreases but stays
above $Q_3(black)$ because we got the one time we got the cheese means we would
always greedily try white again.
$$
Q_4 = 0 + \dfrac{1}{3}(-1 + 0) = - \dfrac{1}{3}
$$
$$
Q_5 =  - \dfrac{1}{3} + \dfrac{1}{4}(-1 + \dfrac{1}{3}) = - \dfrac{1}{2}
$$
$$
Q_6 = - \dfrac{1}{2} + \dfrac{1}{5}(-1 + \dfrac{1}{2}) = - \dfrac{3}{5}
$$

So when do you stop being greedy? How do you choose to switch? When is it optimal
to switch? How can we reason about this tradeoff.


## Regret
Regret tracks missed opportunity at a given timestep. We first find the
optimal action value $v_*$

$$
v_* = \max_{a ∈ A} q(a) = \max_{a} \mathop{\mathbb{E}}[R_t \mid A_t = a]
$$

We treat regret as the opportunity loss from the maximum at a given timestep
$$
v_* - q(A_t)
$$

This maps really well to our intuitive understanding of the concept. I might
regret taking my car to work on a given day instead of the bus. It may have even
been better to bike. Each day you would get new data on this and the optimal may
vary from day to day. So it's a bit messy but over time you learn what gets you
there the fastest.

Our goal now becomes to tradeoff exploration and exploitation by minimizing
total regret. This is just another way to view what we have already say.
Minimizing total regret is the same as maximizing total cumulative reward.

While we can't observe or sample the real regret directly it is a useful
concept to analyze different algorithms.

Regret will continue to grow as we take more steps but the more interesting
question is how fast the regret grows. The greedy strategy we explored above
grows with linear expected total regret. However we can actually find algorithms that growth
with sublinear regret.

## Counting Regret

We define the action regret $\Delta_a$ as the difference between the value and
the optimal value at a given timestep.

$$
\Delta_a = v_* - q(a)
$$

$$
\begin{aligned}
L_t &= \sum^{t}_{i = 1}v_* - q(a)\\
&= \sum_{a ∈ A} N_t(a)(v_* - q(a))\\
&= \sum_{a ∈ A} N_t(a)\Delta_a
\end{aligned}
$$

In other words we can define $L_t$ in a couple of different ways. We can either sum regret at every timestep or equivalently we can sum regret for every action times the amount of times the action has been taken. Not only is this equivalent it is computationally more efficient due to there usually being less actions than timesteps.

What we want is a low count of timesteps with a high gap. This means either
the amount of times we take an action needs to be small or the regret itself.
So how do we ensure this?

## Exploration

We need to explore in order to learn the values. One of the most common ways
to do this is called $\epsilon$-greedy. In this case we have a $\epsilon$
chance that we pick a random action and a $1-\epsilon$ chance we pick a
greedy action. This gives us an exploration strategy in order to not get
stuck in the greedy case. Instead the $\epsilon$ algorithm continues to
explore forever. Since this $\epsilon$ is constant the regret will be linear
throughout the process. While this is much better than the greedy case we can find better algorithms for when to switch from exploration to exploitation.


## Lower Bound

The performance of any algorithm is dependent on how similiar the optimal arm is to other arms. Harder problems have arms with similiar distributions. We can describe this with the action gap and we can find the difference in distributions using KL Divergence. Using the KL divergence Lai and Robbins proved that asymptotic total regret is least logarithmic using:

$$
\lim_{t\to\infin} L_t \geq \log t \sum_{a \mid \Delta_a > 0} \dfrac{\Delta_a}{KL(p(r\mid a) \parallel p(r \mid a_*))}
$$

While the details of this are not too important here the key take away is that the lower bound for performance of a bandit algorithm is logarithmic. Which is much better than linear.

## Upper Confidence Bounds
We often want to make the choice to prioritize exploring uncertain actions
over valuable ones. One way we can achieve this is by being more optimistic.
How we do this in practice is defining a exploration bonus or upper
confidence bound (UCB). This bound means the value $q(a) \leq Q_t(a) +
U_t(a)$ where $U_t(a)$ is the upper bound. In other words we choose an upper
confidence bound such that we can guarantee that the true value will be less
than or equal to our estimate plus the bonus.

We can then have an algorithm that greedily chooses but not based on the estimate but on the estimate + the bonus. This bonus should represent the amount of uncertainty we have about a choice. In other words the bigger the uncertainty the bigger the bonus. This means unexplored actions with low expected values will still be explored due to a bigger bonus.

As a reminder we want to minimize $\sum_a N_t(a)\Delta_a$. So if our regret for the action $\Delta_a$ is large we want the number of time we choose the action to be small and vice versa. Not every $N_t(a)$ can be small because all actions must sum to t. Thus we must prioritize selecting the actions with low gaps over high gaps.

## Hoeffding's Inequality

Hoeffding's inequality lets us bound the difference between expectation and current estimate plus some u. Specifically:

$$
p(E[X] \geq \bar{X}_n + u) \leq e^{-2nu^2}
$$

in terms of the bandit problem this inequality would look like:

$$
p(q(a) \geq Q_t(a) + U_t(a)) \leq e^{-2N_t(a)U_t(a)^2}
$$

Where $N_t(a)$ is the number of steps, $Q_t(a)$ is the current estimate, $q(a)$ is your actual value, and U_t(a) is the bonus at step t.

What this says in practice is there is an exponential curve that maps the likelihood that your estimate is at most a bonus u away from the actual value. The bigger you pick u the less likely that you're true value is larger than estimate plus bonus. He why the function $e ^ {-2N_t(a)U_t(a)^2}$. Has to take $U_t(a)$ in to account.


## Upper Confidence Bound

We can use Hoeffding's Inequality to find the upper confidence bound. We do this with some simple algebra by solving for $U_t(a)$. So given a p that exceed the upper confidence bound, we can use the formula we saw before:

$$
p = e^{-2N_t(a)U_t(a)^2}
$$

We can solve for $U_t(a)$

$$
U_t(a) = \sqrt{\dfrac{-log(p)}{2N_t(a)}}
$$

We can set p based on the number of timesteps that have been observed. This means that as we keep exploring the probability we will underestimate our time bound goes down. This means we will continue to explore but will more often over time pick the optimal value. So far this has just been the basic ideas but now let's explore what this will look like as an algorithm

## UCB Algorithm

$$
a_t = \argmax_{a∈A} Q_t + c \sqrt{\dfrac{log(t)}{2N_t(a)}}
$$

This argmax here just means choose the action at a timestep that maximizes the internal function.

Where c is some hyper-parameter.

What this means in practice is that the bonuses of the most valuable choices will increase over time allowing you to exploit them. However when you choose the value the bonus will drop allowing you to continue to explore. The nice thing about using an exponential curve is once your bonus drops it takes a little while to be high enough to be chosen again due to how gradual the curve is. This means you will select an action either when you are uncertain or when the estimated value is quite high.

This algorithm achieves logarithmic regret. Which means this algorithm is optimal

## Bayesian Bandits

In addition to everything we've done so far we could also learn a model of the environment. This is very simple in this case because there is no environmental state. In fact it would look nearly identical to a value based algorithm. However we can take it a step further and model the distribution of values.

This gives us Bayesian Bandits where we model a distribution over the rewards or $p(R_t \mid \theta, a)$ where $\theta$ is the parameters of your model. For example if you are using a gaussian as your parametric model $\theta$ might just represent the mean and the variance.
$$
p_t(\theta \mid a) \propto p(P_q \mid \theta, a) p_{t-1}(\theta \mid a)
$$
Meaning we estimate the real reward producted with the previous step $\theta$. The symbol $\propto$ is the proportionality symbol which means the lefthand and righthand differ by some constant.

We then would get a distribution for each action. We can then do the same upper confidence bound algorithm as above for each of these individual distributions. In simpler cases we can use something like the standard deviation as this upper bound. Other than this the algorithm is exactly the same as above.

## Probability Matching
Another bayesian algorithm we can do instead of doing an upper bound on distributions is probability matching. In this case we pick action a according to the probability that a is optimal.

$$
\pi_t(a) = p(q(a) = \max_{a'}q(a') | H_{t-1})
$$

This defines a policy per action telling us the probability that we think it is optimal off of prior data $H_{t-1}$. However it can be quite expensive to compute this based on the prior probability.

## Thompson Samping

Instead we can do the above by sampling from each action distribution $Q_t \sim p_t(q(a)), \forall a$.
Given this we can now do the same as above but now with sampling:

$$
\pi_t(a) = E[\mathcal{I}(Q_t(a) = \max_{a'}Q_t(a'))]
$$
In other words wherever the action is the maximum sampled we will include that action in the weighted average.

This can also be expressed similiarly to the probability matching formula above.

$$
\pi_t(a) = p(q(a) = \max_{a'} q(a'))
$$

This algorithm also achieves the logarithmic lower bound on regret as mentioned by Lai and Robbins.

## Markov Decision Process

While we will touch on MDPs much more in the next article I will mention here you can treat the bandits problem as a MDP. By making the state space the knowledge you have so far. So while the environment state does not change your knowledge does. And you can use this to treat the problem like a MDP. This will be covered more in depth later.

## Policy Search
We can also learn a policy $\pi(a)$ directly. So instead of trying to find a value we can find our preferences $H_t(a)$
$$
\pi(a) = \dfrac{e^{H_t(a)}}{\sum_be^{H_t(a)}}
$$

This formula just gives us the softmax of $H_t(a)$. Softmax simply means all your values will sum up to 1. Meaning you can express values as percentages.

These preferences don't necessarily map to values. Instead they are learnable parameters.

So how do we do this? We are going to train a model to update these preferences directly. While this model will not directly learn the values these higher preference will mean we get better values in the long run. The rewards still effect your preferences because we are going to sample rewards and use them to update our preferences. But there is no direct tie.

## Policy Gradients
The general idea is that we are going to update policy parameters in a way that the expected value increases. We will use gradient ascent for this. Like in much of RL ascent works instead of descent because we are maximizing a value instead of minimizing a loss. So for bandits we would do:

$$
\theta = \theta + \alpha \nabla_\theta E[R_t \mid \theta]
$$

Where $\theta$ are the policy parameters. And the nabla $\nabla$ refers to the gradient. This is just a simple gradient ascent algorithm with a learning rate $\alpha$.

The question then becomes how do we calculate this gradient? We don't have the expectation readily available so we have no idea how to calculate the gradient of it.

## Gradient Bandits
So how do we turn the above formula $\nabla_\theta E[R_t \mid \theta]$ into something we can readily use? We can use the Log-likelihood trick (or REINFORCE trick) to convert this formula into something we can actually sample.