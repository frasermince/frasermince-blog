---
title: "Exploration and Exploitation"
date: "2020-11-11T10:00:00.0000"
template: "post"
draft: false
slug: "reinforcement-learning-2"
category: "Machine Learning"
tags:
  - "Machine Learning"
  - "Reinforcement Learning"
description: ""
socialImage: "/media/image-2.jpg"
---

Moving past the basics I now want to explore a tradeoff in Reinforcement
Learning. This is the tradeoff between exploration and exploitation. Whenever we
are dealing with a reinforcement learning there is a choice we have to make. Do
we take advantage of the most valuable choices we know about, or do we instead
search for new, potentially more valuable choices. Through exploitation we can
maximize our performance of the problem but through exploration we can increase
our knowledge.

We can depict how we make this tradeoff by simplifying our environment. We do
this by taking away the sequential structure of our environment and assume that
past actions do not effect our current environment state.

For example we have a rat that can pull one of two levers. When he pulls a lever
he either gets an electric shock or some cheese. Each lever has a certain
probability of giving a shock and a certain probability of giving cheese. Let's
say the rat pulls the first lever on the first day and gets a shock. He pulls
the second layer on the second day and gets some cheese. On the third day he
again pulls the second layer and gets some shock. What should he do next? While
you may have a theory for what the rat should do next the answer is non trivial.
Should the rat keep going to the lever that led to a reward once? Or further
explore the one that hasn't given a reward yet?

As we consider this tradeoff it becomes clear to maximize the long term strategy
we may have to make short term sacrifices. For example we may at times not
choose the decision we think is optimal in order to gather new information. This
is also a byproduct of online decision making. Where we are collecting data and
making decisions at the same time.

We call the setting in which we discuss this tradeoff the Multi-Armed Bandit.
While this may sound like some boss fight in a Final Fantasy game the word
bandit here comes from one armed bandit which is another name for a slot
machine. The basic idea of the multi-armed bandit is a row of slot machines each
having it's own fixed distribution that is unknown. Based on these distributions
the slot machines will payoff. We view which slot machine to play at a given
timestep as the action choice. Once we choose a slot machine we get some amount
of reward this reward is assumed to be random, hence our distribution. One nice
thing about this problem is it simplifies the problem of RL by not worrying
about sequentially or what actions were taken at previous timesteps. This is due
to the fixed nature of the distributions. This information ceases to matter in
this problem. Our goal is to maximize not rewards at any given timestep but
instead cumulative rewards. What this means in practice is we don't want to lose
too much value in the learning process. We want to be able to learn in the
optimal way possible.
