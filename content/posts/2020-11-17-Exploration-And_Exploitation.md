---
title: "Exploration and Exploitation"
date: "2020-11-11T10:00:00.0000"
template: "post"
draft: false
slug: "reinforcement-learning-2"
category: "Machine Learning"
tags:
  - "Machine Learning"
  - "Reinforcement Learning"
description: ""
socialImage: "/media/image-2.jpg"
---

Moving past the basics I now want to explore a tradeoff in Reinforcement
Learning. This is the tradeoff between exploration and exploitation. Whenever we
are dealing with a reinforcement learning there is a choice we have to make. Do
we take advantage of the most valuable choices we know about, or do we instead
search for new, potentially more valuable choices. Through exploitation we can
maximize our performance of the problem but through exploration we can increase
our knowledge.

We can depict how we make this tradeoff by simplifying our environment. We do
this by taking away the sequential structure of our environment and assume that
past actions do not effect our current environment state.

For example we have a rat that can pull one of two levers. When he pulls a lever
he either gets an electric shock or some cheese. Each lever has a certain
probability of giving a shock and a certain probability of giving cheese. Let's
say the rat pulls the first lever on the first day and gets a shock. He pulls
the second layer on the second day and gets some cheese. On the third day he
again pulls the second layer and gets some shock. What should he do next? While
you may have a theory for what the rat should do next the answer is non trivial.
Should the rat keep going to the lever that led to a reward once? Or further
explore the one that hasn't given a reward yet?

As we consider this tradeoff it becomes clear to maximize the long term strategy
we may have to make short term sacrifices. For example we may at times not
choose the decision we think is optimal in order to gather new information. This
is also a byproduct of online decision making. Where we are collecting data and
making decisions at the same time.

We call the setting in which we discuss this tradeoff the Multi-Armed Bandit.
While this may sound like some boss fight in a Final Fantasy game the word
bandit here comes from one armed bandit which is another name for a slot
machine. The basic idea of the multi-armed bandit is a row of slot machines each
having it's own fixed distribution that is unknown. Based on these distributions
the slot machines will payoff. We view which slot machine to play at a given
timestep as the action choice. Once we choose a slot machine we get some amount
of reward this reward is assumed to be random, hence our distribution. One nice
thing about this problem is it simplifies the problem of RL by not worrying
about sequentially or what actions were taken at previous timesteps. This is due
to the fixed nature of the distributions. This information ceases to matter in
this problem. Our goal is to maximize not rewards at any given timestep but
instead cumulative rewards. What this means in practice is we don't want to lose
too much value in the learning process. We want to be able to learn in the
optimal way possible.

Due to not having any environmental state to inform our action we can simplify
the q_a value to just being condition on action and not on state. This could
also be expressed as finding values only where an action is chosen.

$$
q(a) = E[R_t | A_t = a]
$$

$$
Q_t(a) = \dfrac{\sum_{n=1}^{t}R_n\mathcal{I}(A_n = a)}{\sum_{n=1}^{t}\mathcal{I}(A_n = a)}
$$

This $\mathcal{I}$ refers to the indicator function. This function is 1 when the action
occurs and 0 otherwise. This means that we only look at values where the action
occurs.

This can also be treated incrementally.
$$
Q_t(A_t) = Q_{t-l}(A_t) + α_t(R_t - Q_{t-1}(A_t))
$$
This $Q_t$ no longer refers to the true value but rather an estimate.

The α is the step size. Often we set the step size to 1/number of steps. This
gives us an average. We could also use a constant step size meaning we converge
towards a value instead of averaging. This is useful for function
approximations like neural nets.


In this case at every step we are finding the difference between our reward
currently and our past value. This acts as an error function when multiplied by
the step size alpha and added to the current previous value. 
This is derived using simple algebra starting with:
$$
Q_{n+1} = \dfrac{1}{n}\sum_{i=1}^{n}(R_i)
$$
And solving to define $Q_{n+1}$ in terms of $Q_n$. I won't repeat this process
here but if you are interested in how the above incremental formula is derived I
strongly suggest you check out page 24 of [Sutton And Barto](http://incompleteideas.net/book/bookdraft2017nov5.pdf).

The next dilemma we have to face is over when do we stop greedily getting what
seems like the best value?

Cheese = 1
Shock = -1

1. Pull black lever: shock
2. Pull white lever: cheese
3. Pull white lever: shock

$$
Q_3(white) = 0
$$
$$
Q_3(black) = -1
$$

Now let's assume the following:
4. Pull white lever: shock
5. Pull white lever: shock
6. Pull white lever: shock

We now have:

$$
Q_3(white) = 0
$$
$$
Q_3(black) = -1
$$

You will notice as we are shocked more the $Q_3(white)$ decreases but stays
above $Q_3(black)$ because we got the one time we got the cheese means we would
always greedily try white again.
$$
Q_4 = 0 + \dfrac{1}{3}(-1 + 0) = - \dfrac{1}{3}
$$
$$
Q_5 =  - \dfrac{1}{3} + \dfrac{1}{4}(-1 + \dfrac{1}{3}) = - \dfrac{1}{2}
$$
$$
Q_6 = - \dfrac{1}{2} + \dfrac{1}{5}(-1 + \dfrac{1}{2}) = - \dfrac{3}{5}
$$

So when do you stop being greedy? How do you choose to switch? When is it optimal
to switch? How can we reason about this tradeoff.


## Regret
Regret tracks missed opportunity at a given timestep. We first find the
optimal action value $ v_* $

$$
v_* = \max_{a ∈ A} q(a) = \max_{a} \mathop{\mathbb{E}}[R_t \mid A_t = a]
$$

We treat regret as the opportunity loss from the maximum at a given timestep
$$
v_* - q(A_t)
$$

This maps really well to our intuitive understanding of the concept. I might
regret taking my car to work on a given day instead of the bus. It may have even
been better to bike. Each day you would get new data on this and the optimal may
vary from day to day. So it's a bit messy but over time you learn what gets you
there the fastest.

Our goal now becomes to tradeoff exploration and exploitation by minimizing
total regret. This is just another way to view what we have already say.
Minimizing total regret is the same as maximizing total cumulative reward.

While we can't observe or sample the real regret directly it is a useful
concept to analyze different algorithms.

Regret will continue to grow as we take more steps but the more interesting
question is how fast the regret grows. The greedy strategy we explored above
grows with linear expected total regret. However we can actually find algorithms that growth
with sublinear regret.

## Counting Regret

We define the action regret $\Delta_a$ as the difference between the value and
the optimal value at a given timestep.

$$
\Delta_a = v_* - q(a)
$$

$$
\begin{aligned}
L_t &= \sum^{t}_{i = 1}v_* - q(a)\\
&= \sum_{a ∈ A} N_t(a)(v_* - q(a))\\
&= \sum_{a ∈ A} N_t(a)\Delta_a
\end{aligned}
$$

In other words we can define $L_t$ in a couple of different ways. We can either sum regret at every timestep or equivalently we can sum regret for every action times the amount of times the action has been taken. Not only is this equivalent it is computationally more efficient due to there usually being less actions than timesteps.

What we want is a low count of timesteps with a high gap. This means either
the amount of times we take an action needs to be small or the regret itself.
So how do we ensure this?

## Exploration

We need to explore in order to learn the values. One of the most common ways
to do this is called $\epsilon$-greedy. In this case we have a $\epsilon$
chance that we pick a random action and a $1-\epsilon$ chance we pick a
greedy action. This gives us an exploration strategy in order to not get
stuck in the greedy case. Instead the $\epsilon$ algorithm continues to
explore forever. Since this $\epsilon$ is constant the regret will be linear
throughout the process. While this is much better than the greedy case we can find better algorithms for when to switch from exploration to exploitation.


## Lower Bound

The performance of any algorithm is dependent on how similiar the optimal arm is to other arms. Harder problems have arms with similiar distributions. We can describe this with the action gap and we can find the difference in distributions using KL Divergence. Using the KL divergence Lai and Robbins proved that asymptotic total regret is least logarithmic using:

$$
\lim_{t\to\infin} L_t \geq \log t \sum_{a \mid \Delta_a > 0} \dfrac{\Delta_a}{KL(p(r\mid a) \parallel p(r \mid a_*))}
$$

While the details of this are not too important here the key take away is that the lower bound for performance of a bandit algorithm is logarithmic. Which is much better than linear.

## Upper Confidence Bounds
We often want to make the choice to prioritize exploring uncertain actions
over valuable ones. One way we can achieve this is by being more optimistic.
How we do this in practice is defining a exploration bonus or upper
confidence bound (UCB). This bound means the value $q(a) \leq Q_t(a) +
U_t(a)$ where $U_t(a)$ is the upper bound. In other words we choose an upper
confidence bound such that we can guarantee that the true value will be less
than or equal to our estimate plus the bonus.

We can then have an algorithm that greedily chooses but not based on the estimate but on the estimate + the bonus. This bonus should represent the amount of uncertainty we have about a choice. In other words the bigger the uncertainty the bigger the bonus. This means unexplored actions with low expected values will still be explored due to a bigger bonus.

As a reminder we want to minimize $\sum_a N_t(a)\Delta_a$. So if our regret for the action $\Delta_a$ is large we want the number of time we choose the action to be small and vice versa. Not every $N_t(a)$ can be small because all actions must sum to t. Thus we must prioritize selecting the actions with low gaps over high gaps.