---
title: "Exploration and Exploitation"
date: "2020-11-11T10:00:00.0000"
template: "post"
draft: false
slug: "reinforcement-learning-2"
category: "Machine Learning"
tags:
  - "Machine Learning"
  - "Reinforcement Learning"
description: ""
socialImage: "/media/image-2.jpg"
---

Moving past the basics I now want to explore a tradeoff in Reinforcement
Learning. This is the tradeoff between exploration and exploitation. Whenever we
are dealing with a reinforcement learning there is a choice we have to make. Do
we take advantage of the most valuable choices we know about, or do we instead
search for new, potentially more valuable choices. Through exploitation we can
maximize our performance of the problem but through exploration we can increase
our knowledge.

We can depict how we make this tradeoff by simplifying our environment. We do
this by taking away the sequential structure of our environment and assume that
past actions do not effect our current environment state.

For example we have a rat that can pull one of two levers. When he pulls a lever
he either gets an electric shock or some cheese. Each lever has a certain
probability of giving a shock and a certain probability of giving cheese. Let's
say the rat pulls the first lever on the first day and gets a shock. He pulls
the second layer on the second day and gets some cheese. On the third day he
again pulls the second layer and gets some shock. What should he do next?
